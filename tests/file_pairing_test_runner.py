#!/usr/bin/env python3
"""
TEST RUNNER DLA PAROWANIA PLIKÃ“W
Specjalizowany runner do testowania i optymalizacji funkcji parowania plikÃ³w
"""

import json
import os
import shutil
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Add src to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from tests.automated_optimization_system import (
    AutomatedOptimizationSystem,
    OptimizationResult,
)
from tests.file_pairing_optimization_strategies import (
    FilePairingOptimizationLibrary,
    FilePairingTestEnvironment,
)


class FilePairingTestRunner:
    """Specjalizowany test runner dla parowania plikÃ³w"""

    def __init__(self):
        self.project_root = project_root
        self.results_dir = project_root / "file_pairing_test_results"
        self.results_dir.mkdir(exist_ok=True)

        # Komponenty
        self.optimization_library = FilePairingOptimizationLibrary()
        self.test_environment = FilePairingTestEnvironment()
        self.optimization_system = AutomatedOptimizationSystem()

        # Konfiguracja testÃ³w
        self.test_scenarios = [
            "small_best_match",  # 50 plikÃ³w, best_match
            "medium_first_match",  # 300 plikÃ³w, first_match
            "large_best_match",  # 1000 plikÃ³w, best_match
        ]

        print("ğŸ”§ Inicjalizacja File Pairing Test Runner...")
        print(f"ğŸ“ Wyniki bÄ™dÄ… zapisywane w: {self.results_dir}")

    def run_comprehensive_pairing_tests(self) -> Dict[str, Any]:
        """Uruchamia kompleksowe testy parowania plikÃ³w"""
        session_id = f"pairing_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"\nğŸš€ RozpoczÄ™cie testÃ³w parowania plikÃ³w: {session_id}")

        results = {
            "session_id": session_id,
            "start_time": datetime.now().isoformat(),
            "test_scenarios": self.test_scenarios,
            "baseline_results": {},
            "optimization_results": [],
            "final_analysis": {},
            "recommendations": [],
        }

        try:
            # FAZA 1: Testy bazowe
            print("\nğŸ“Š FAZA 1: Testy bazowe...")
            baseline_results = self._run_baseline_tests()
            results["baseline_results"] = baseline_results

            # FAZA 2: Optymalizacje
            print("\nğŸ”§ FAZA 2: Optymalizacje...")
            optimization_results = self._run_optimization_tests(baseline_results)
            results["optimization_results"] = optimization_results

            # FAZA 3: Analiza wynikÃ³w
            print("\nğŸ“ˆ FAZA 3: Analiza wynikÃ³w...")
            final_analysis = self._analyze_optimization_results(
                baseline_results, optimization_results
            )
            results["final_analysis"] = final_analysis

            # FAZA 4: Rekomendacje
            print("\nğŸ’¡ FAZA 4: Generowanie rekomendacji...")
            recommendations = self._generate_recommendations(results)
            results["recommendations"] = recommendations

            print(f"\nâœ… Testy parowania zakoÅ„czone: {session_id}")

        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas testÃ³w: {e}")
            results["error"] = str(e)

        finally:
            results["end_time"] = datetime.now().isoformat()
            self._save_test_results(results)

        return results

    def _run_baseline_tests(self) -> Dict[str, Any]:
        """Uruchamia testy bazowe bez optymalizacji"""
        baseline_results = {}

        for scenario in self.test_scenarios:
            print(f"  ğŸ” Testowanie scenariusza: {scenario}")

            # Pobierz konfiguracjÄ™ scenariusza
            config = self.test_environment.create_test_scenario(scenario)

            # Uruchom test
            metrics = self.test_environment.measure_pairing_performance(
                config["file_count"], config["pairing_strategy"]
            )

            baseline_results[scenario] = {
                "config": config,
                "metrics": metrics,
                "timestamp": datetime.now().isoformat(),
            }

            print(
                f"    âœ… {scenario}: {metrics['pairs_created']} par w {metrics['pairing_time_ms']:.2f}ms"
            )

        return baseline_results

    def _run_optimization_tests(
        self, baseline_results: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """Uruchamia testy z optymalizacjami"""
        optimization_results = []

        # Przygotuj 2 rewizje kodu
        revisions = self._prepare_code_revisions()

        for i, revision in enumerate(revisions, 1):
            print(f"\n  ğŸ”§ Rewizja {i}: {revision['name']}")

            try:
                # UtwÃ³rz backup
                backup_path = self.optimization_system.create_backup(f"pairing_rev_{i}")

                # Zastosuj optymalizacjÄ™
                success = self.optimization_system.apply_optimization(
                    revision["type"], revision["target_file"], revision["changes"]
                )

                if not success:
                    print(f"    âŒ Nie udaÅ‚o siÄ™ zastosowaÄ‡ rewizji {i}")
                    continue

                # Uruchom testy dla kaÅ¼dego scenariusza
                revision_results = {
                    "revision_id": f"rev_{i}",
                    "revision_name": revision["name"],
                    "revision_type": revision["type"],
                    "scenario_results": {},
                    "overall_improvement": 0.0,
                }

                total_improvement = 0.0
                scenario_count = 0

                for scenario in self.test_scenarios:
                    config = baseline_results[scenario]["config"]

                    # Uruchom test z optymalizacjÄ…
                    metrics = self.test_environment.measure_pairing_performance(
                        config["file_count"], config["pairing_strategy"]
                    )

                    # Oblicz poprawÄ™
                    baseline_metrics = baseline_results[scenario]["metrics"]
                    improvement = self._calculate_improvement(
                        baseline_metrics["pairing_time_ms"], metrics["pairing_time_ms"]
                    )

                    revision_results["scenario_results"][scenario] = {
                        "metrics": metrics,
                        "improvement_percent": improvement,
                        "baseline_metrics": baseline_metrics,
                    }

                    total_improvement += improvement
                    scenario_count += 1

                if scenario_count > 0:
                    revision_results["overall_improvement"] = (
                        total_improvement / scenario_count
                    )

                optimization_results.append(revision_results)

                print(
                    f"    âœ… Rewizja {i} zakoÅ„czona: {revision_results['overall_improvement']:.2f}% poprawa"
                )

                # PrzywrÃ³Ä‡ backup
                self._restore_backup(backup_path)

            except Exception as e:
                print(f"    âŒ BÅ‚Ä…d podczas rewizji {i}: {e}")

                # PrzywrÃ³Ä‡ backup w przypadku bÅ‚Ä™du
                if "backup_path" in locals():
                    self._restore_backup(backup_path)

        return optimization_results

    def _prepare_code_revisions(self) -> List[Dict[str, Any]]:
        """Przygotowuje 2 rewizje kodu do testowania"""
        return [
            {
                "name": "Trie Memory Optimization",
                "type": "memory_optimization",
                "target_file": "src/logic/file_pairing.py",
                "changes": {
                    "optimizations": [
                        {
                            "type": "trie_memory_optimization",
                            "max_size": 5000,
                            "cleanup_threshold": 0.8,
                            "memory_limit_mb": 50,
                        }
                    ],
                    "description": [
                        "Zmniejszenie maksymalnego rozmiaru Trie z 10000 na 5000",
                        "Dodanie automatycznego cleanup przy 80% wykorzystania",
                        "Limit pamiÄ™ci 50MB dla struktury Trie",
                    ],
                },
            },
            {
                "name": "File Info Cache Optimization",
                "type": "cache_optimization",
                "target_file": "src/logic/file_pairing.py",
                "changes": {
                    "optimizations": [
                        {
                            "type": "file_info_cache",
                            "cache_size": 10000,
                            "ttl_seconds": 300,
                            "cache_key": "file_info",
                        }
                    ],
                    "description": [
                        "Cache dla FileInfo obiektÃ³w (max 10000 wpisÃ³w)",
                        "TTL 5 minut dla cache",
                        "Unikanie ponownego parsowania rozszerzeÅ„ plikÃ³w",
                    ],
                },
            },
        ]

    def _calculate_improvement(
        self, baseline_value: float, optimized_value: float
    ) -> float:
        """Oblicza procentowÄ… poprawÄ™"""
        if baseline_value <= 0:
            return 0.0

        # Dla metryk czasowych - mniejsze jest lepsze
        improvement = ((baseline_value - optimized_value) / baseline_value) * 100
        return max(improvement, -100.0)  # Ogranicz do -100%

    def _restore_backup(self, backup_path: str):
        """Przywraca kod z backup"""
        try:
            backup_dir = Path(backup_path)
            if backup_dir.exists():
                # PrzywrÃ³Ä‡ pliki
                for file_path in backup_dir.rglob("*"):
                    if file_path.is_file():
                        relative_path = file_path.relative_to(backup_dir)
                        target_path = self.project_root / relative_path
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(file_path, target_path)

                print(f"    ğŸ”„ PrzywrÃ³cono backup: {backup_path}")
        except Exception as e:
            print(f"    âŒ BÅ‚Ä…d podczas przywracania backup: {e}")

    def _analyze_optimization_results(
        self,
        baseline_results: Dict[str, Any],
        optimization_results: List[Dict[str, Any]],
    ) -> Dict[str, Any]:
        """Analizuje wyniki optymalizacji"""
        analysis = {
            "best_revision": None,
            "average_improvement": 0.0,
            "scenario_analysis": {},
            "recommendations": [],
        }

        if not optimization_results:
            return analysis

        # ZnajdÅº najlepszÄ… rewizjÄ™
        best_revision = max(
            optimization_results, key=lambda x: x["overall_improvement"]
        )
        analysis["best_revision"] = {
            "revision_id": best_revision["revision_id"],
            "revision_name": best_revision["revision_name"],
            "improvement": best_revision["overall_improvement"],
        }

        # Oblicz Å›redniÄ… poprawÄ™
        total_improvement = sum(
            rev["overall_improvement"] for rev in optimization_results
        )
        analysis["average_improvement"] = total_improvement / len(optimization_results)

        # Analiza per scenariusz
        for scenario in self.test_scenarios:
            scenario_improvements = []
            for rev in optimization_results:
                if scenario in rev["scenario_results"]:
                    improvement = rev["scenario_results"][scenario][
                        "improvement_percent"
                    ]
                    scenario_improvements.append(improvement)

            if scenario_improvements:
                analysis["scenario_analysis"][scenario] = {
                    "best_improvement": max(scenario_improvements),
                    "average_improvement": sum(scenario_improvements)
                    / len(scenario_improvements),
                    "improvement_count": len(
                        [i for i in scenario_improvements if i > 0]
                    ),
                }

        return analysis

    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """Generuje rekomendacje na podstawie wynikÃ³w"""
        recommendations = []

        if "final_analysis" not in results:
            return recommendations

        analysis = results["final_analysis"]

        # Rekomendacje na podstawie najlepszej rewizji
        if analysis.get("best_revision"):
            best_rev = analysis["best_revision"]
            recommendations.append(
                f"Zastosuj rewizjÄ™ '{best_rev['revision_name']}' - "
                f"poprawa {best_rev['improvement']:.1f}%"
            )

        # Rekomendacje na podstawie Å›redniej poprawy
        avg_improvement = analysis.get("average_improvement", 0)
        if avg_improvement > 20:
            recommendations.append(
                "Optymalizacje wykazaÅ‚y znaczÄ…cÄ… poprawÄ™ - kontynuuj rozwÃ³j"
            )
        elif avg_improvement > 10:
            recommendations.append(
                "Optymalizacje wykazaÅ‚y umiarkowanÄ… poprawÄ™ - rozwaÅ¼ dodatkowe strategie"
            )
        else:
            recommendations.append(
                "Optymalizacje wykazaÅ‚y minimalnÄ… poprawÄ™ - przeanalizuj inne podejÅ›cia"
            )

        # Rekomendacje per scenariusz
        for scenario, scenario_analysis in analysis.get(
            "scenario_analysis", {}
        ).items():
            if scenario_analysis["best_improvement"] > 30:
                recommendations.append(
                    f"Scenariusz {scenario} wykazaÅ‚ doskonaÅ‚Ä… poprawÄ™ - "
                    f"rozwaÅ¼ specjalizacjÄ™ dla tego typu danych"
                )

        return recommendations

    def _save_test_results(self, results: Dict[str, Any]):
        """Zapisuje wyniki testÃ³w"""
        try:
            # Zapisz gÅ‚Ã³wny wynik
            results_file = (
                self.results_dir / f"pairing_test_{results['session_id']}.json"
            )
            with open(results_file, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            # Wygeneruj raport
            self._generate_test_report(results)

            print(f"ğŸ’¾ Zapisano wyniki testÃ³w: {results_file}")

        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas zapisywania wynikÃ³w: {e}")

    def _generate_test_report(self, results: Dict[str, Any]):
        """Generuje raport z testÃ³w"""
        try:
            report_file = (
                self.results_dir / f"pairing_report_{results['session_id']}.md"
            )

            with open(report_file, "w", encoding="utf-8") as f:
                f.write(
                    f"# Raport TestÃ³w Parowania PlikÃ³w: {results['session_id']}\n\n"
                )
                f.write(f"**Data rozpoczÄ™cia:** {results['start_time']}\n")
                f.write(f"**Data zakoÅ„czenia:** {results['end_time']}\n\n")

                f.write("## Scenariusze Testowe\n\n")
                for scenario in results["test_scenarios"]:
                    f.write(f"- {scenario}\n")
                f.write("\n")

                f.write("## Wyniki Bazowe\n\n")
                for scenario, data in results["baseline_results"].items():
                    metrics = data["metrics"]
                    f.write(f"### {scenario}\n\n")
                    f.write(
                        f"- **Czas parowania:** {metrics['pairing_time_ms']:.2f}ms\n"
                    )
                    f.write(f"- **Utworzone pary:** {metrics['pairs_created']}\n")
                    f.write(f"- **Przetworzone pliki:** {metrics['files_processed']}\n")
                    f.write(f"- **Stosunek par:** {metrics['pair_ratio']:.2f}\n")
                    f.write(
                        f"- **Pary/sekundÄ™:** {metrics['pairs_per_second']:.2f}\n\n"
                    )

                f.write("## Wyniki Optymalizacji\n\n")
                for rev in results["optimization_results"]:
                    f.write(f"### {rev['revision_name']}\n\n")
                    f.write(
                        f"- **OgÃ³lna poprawa:** {rev['overall_improvement']:.2f}%\n\n"
                    )

                    for scenario, scenario_data in rev["scenario_results"].items():
                        f.write(f"#### {scenario}\n")
                        f.write(
                            f"- **Poprawa:** {scenario_data['improvement_percent']:.2f}%\n"
                        )
                        f.write(
                            f"- **Nowy czas:** {scenario_data['metrics']['pairing_time_ms']:.2f}ms\n"
                        )
                        f.write(
                            f"- **Nowe pary/sekundÄ™:** {scenario_data['metrics']['pairs_per_second']:.2f}\n\n"
                        )

                if "final_analysis" in results:
                    f.write("## Analiza KoÅ„cowa\n\n")
                    analysis = results["final_analysis"]

                    if analysis.get("best_revision"):
                        best = analysis["best_revision"]
                        f.write(
                            f"**Najlepsza rewizja:** {best['revision_name']} ({best['improvement']:.2f}%)\n\n"
                        )

                    f.write(
                        f"**Åšrednia poprawa:** {analysis.get('average_improvement', 0):.2f}%\n\n"
                    )

                if "recommendations" in results:
                    f.write("## Rekomendacje\n\n")
                    for i, rec in enumerate(results["recommendations"], 1):
                        f.write(f"{i}. {rec}\n")
                    f.write("\n")

            print(f"ğŸ“Š Wygenerowano raport: {report_file}")

        except Exception as e:
            print(f"âŒ BÅ‚Ä…d podczas generowania raportu: {e}")


def main():
    """GÅ‚Ã³wna funkcja uruchamiajÄ…ca testy parowania"""
    print("ğŸš€ TEST RUNNER DLA PAROWANIA PLIKÃ“W")
    print("=" * 50)

    # Uruchom testy
    runner = FilePairingTestRunner()
    results = runner.run_comprehensive_pairing_tests()

    # Podsumowanie
    print("\n" + "=" * 50)
    print("ğŸ“Š PODSUMOWANIE TESTOW PAROWANIA")
    print("=" * 50)

    if "error" in results:
        print(f"âŒ Testy zakoÅ„czone z bÅ‚Ä™dem: {results['error']}")
    else:
        print(f"âœ… Testy zakoÅ„czone pomyÅ›lnie")

        # PokaÅ¼ najlepszÄ… rewizjÄ™
        if "final_analysis" in results and results["final_analysis"].get(
            "best_revision"
        ):
            best = results["final_analysis"]["best_revision"]
            print(f"ğŸ† Najlepsza rewizja: {best['revision_name']}")
            print(f"ğŸ“ˆ Poprawa: {best['improvement']:.2f}%")

        # PokaÅ¼ Å›redniÄ… poprawÄ™
        if "final_analysis" in results:
            avg = results["final_analysis"].get("average_improvement", 0)
            print(f"ğŸ“Š Åšrednia poprawa: {avg:.2f}%")

        # PokaÅ¼ rekomendacje
        if results.get("recommendations"):
            print(f"ğŸ’¡ Rekomendacje: {len(results['recommendations'])}")

    print(f"\nğŸ“ Wyniki zapisane w: {runner.results_dir}")


if __name__ == "__main__":
    main()
