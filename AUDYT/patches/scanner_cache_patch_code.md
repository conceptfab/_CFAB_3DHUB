# üîß PATCH CODE: scanner_cache.py

> **Plik docelowy:** `src/logic/scanner_cache.py`  
> **Priorytet:** üî¥üî¥üî¥ WYSOKIE - Cache wynik√≥w skanowania  
> **Data utworzenia:** 2025-01-28  
> **Szacowany czas implementacji:** 4-6 dni roboczych

## üéØ CELE PATCHY

### G≈Ç√≥wne optymalizacje:
1. **Optymalizacja cleanup algorithm** - O(n¬≤) ‚Üí O(n) complexity
2. **Memory monitoring** - kontrola u≈ºycia pamiƒôci cache
3. **Comprehensive statistics** - agregacja statystyk obu cache
4. **Pattern-based removal** - efektywne usuwanie grupowe

### Oczekiwane rezultaty:
- ‚ö° **80% szybsze** operacje cleanup
- üìä **Pe≈Çny monitoring** pamiƒôci cache
- üéØ **95%+ hit ratio** maintenance

---

## üìã IMPLEMENTACJA PATCH

### 1. **OPTYMALIZACJA CLEANUP ALGORITHM**

**Lokalizacja:** Linie 238-265 - `_cleanup_cache_by_age_and_size()`

```python
def _cleanup_cache_by_age_and_size(self, cache_obj, current_time, cache_name):
    """Optymalizowane cleanup w jednym przej≈õciu O(n) complexity."""
    to_remove_age = set()
    valid_entries = []
    
    # Single pass - segregacja wpis√≥w O(n)
    for key, (timestamp, value) in cache_obj.cache.items():
        if current_time - timestamp > MAX_CACHE_AGE_SECONDS:
            to_remove_age.add(key)
        else:
            valid_entries.append((key, timestamp))
    
    # Usuwanie nadmiaru rozmiaru
    to_remove_size = set()
    if len(valid_entries) > MAX_CACHE_ENTRIES:
        # Sort tylko valid entries
        valid_entries.sort(key=lambda x: x[1])  # Sortuj po timestamp
        size_limit = MAX_CACHE_ENTRIES
        # Usu≈Ñ najstarsze z valid entries
        for key, _ in valid_entries[:-size_limit]:
            to_remove_size.add(key)
    
    # Batch removal - jedna operacja
    all_to_remove = to_remove_age | to_remove_size
    removed_count = 0
    for key in all_to_remove:
        if key in cache_obj.cache:
            del cache_obj.cache[key]
            removed_count += 1
    
    if removed_count > 0:
        logger.debug(
            f"Usuniƒôto {removed_count} wpis√≥w z {cache_name} cache "
            f"(wiek: {len(to_remove_age)}, rozmiar: {len(to_remove_size)})"
        )
```

### 2. **MEMORY MONITORING**

**Lokalizacja:** Dodaƒá po linii 292 - nowe metody

```python
def _estimate_scan_result_size(self, scan_result) -> int:
    """Szacuje rozmiar wyniku skanowania w bajtach."""
    import sys
    
    file_pairs, unpaired_archives, unpaired_previews, special_folders = scan_result
    
    size = 0
    # FilePair objects
    for pair in file_pairs:
        size += sys.getsizeof(pair)
        size += sys.getsizeof(pair.archive_file) if pair.archive_file else 0
        size += sys.getsizeof(pair.preview_file) if pair.preview_file else 0
    
    # String lists
    size += sum(sys.getsizeof(s) for s in unpaired_archives)
    size += sum(sys.getsizeof(s) for s in unpaired_previews)
    
    # SpecialFolder objects
    for folder in special_folders:
        size += sys.getsizeof(folder)
    
    return size

def get_cache_memory_usage(self) -> Dict[str, float]:
    """Monitoring pamiƒôci cache."""
    import sys
    
    file_map_size = 0
    for key, (timestamp, value) in self.file_map_cache.cache.items():
        file_map_size += sys.getsizeof(key)
        file_map_size += sys.getsizeof(timestamp)
        file_map_size += sys.getsizeof(value)
        # Dodaj rozmiar zawarto≈õci dict
        for k, v in value.items():
            file_map_size += sys.getsizeof(k) + sys.getsizeof(v)
            file_map_size += sum(sys.getsizeof(item) for item in v)
    
    scan_result_size = 0
    for key, (timestamp, value) in self.scan_result_cache.cache.items():
        scan_result_size += sys.getsizeof(key)
        scan_result_size += sys.getsizeof(timestamp)
        scan_result_size += self._estimate_scan_result_size(value)
    
    return {
        "file_map_memory_mb": file_map_size / (1024 * 1024),
        "scan_result_memory_mb": scan_result_size / (1024 * 1024),
        "total_memory_mb": (file_map_size + scan_result_size) / (1024 * 1024)
    }

def check_memory_pressure(self) -> bool:
    """Sprawdza czy cache u≈ºywa za du≈ºo pamiƒôci."""
    memory_usage = self.get_cache_memory_usage()
    # Limit 100MB dla ca≈Çego cache
    return memory_usage["total_memory_mb"] > 100.0

def adaptive_cleanup(self):
    """Adaptacyjne czyszczenie przy presji pamiƒôci."""
    if self.check_memory_pressure():
        logger.warning("Wykryto presjƒô pamiƒôci cache - wymuszone czyszczenie")
        # Zmniejsz tymczasowo limity
        old_limit = MAX_CACHE_ENTRIES
        reduced_limit = int(old_limit * 0.7)  # Zmniejsz o 30%
        
        # Cleanup z obni≈ºonym limitem
        current_time = time.time()
        self._cleanup_cache_by_age_and_size(
            self.file_map_cache, current_time, "file_map", reduced_limit
        )
        self._cleanup_cache_by_age_and_size(
            self.scan_result_cache, current_time, "scan_result", reduced_limit
        )
```

### 3. **COMPREHENSIVE STATISTICS**

**Lokalizacja:** ZastƒÖpiƒá metodƒô `get_statistics()` (linie 267-292)

```python
def _get_cache_stats(self, cache_obj) -> Dict[str, any]:
    """Pobiera statystyki dla pojedynczego cache."""
    total_requests = cache_obj.cache_hits + cache_obj.cache_misses
    hit_ratio = 0
    if total_requests > 0:
        hit_ratio = (cache_obj.cache_hits / total_requests) * 100
    
    return {
        "entries": len(cache_obj.cache),
        "hits": cache_obj.cache_hits,
        "misses": cache_obj.cache_misses,
        "total_requests": total_requests,
        "hit_ratio": hit_ratio
    }

def get_statistics(self) -> Dict[str, any]:
    """Zwraca kompleksowe statystyki cache."""
    with self._lock:
        file_map_stats = self._get_cache_stats(self.file_map_cache)
        scan_result_stats = self._get_cache_stats(self.scan_result_cache)
        memory_stats = self.get_cache_memory_usage()
        
        # Agregacja statystyk
        combined_stats = {
            "total_entries": file_map_stats["entries"] + scan_result_stats["entries"],
            "total_hits": file_map_stats["hits"] + scan_result_stats["hits"],
            "total_misses": file_map_stats["misses"] + scan_result_stats["misses"],
            "total_requests": file_map_stats["total_requests"] + scan_result_stats["total_requests"],
            "average_hit_ratio": 0,
            "memory_pressure": self.check_memory_pressure()
        }
        
        if combined_stats["total_requests"] > 0:
            combined_stats["average_hit_ratio"] = (
                combined_stats["total_hits"] / combined_stats["total_requests"]
            ) * 100
        
        stats = {
            "file_map": file_map_stats,
            "scan_result": scan_result_stats,
            "combined": combined_stats,
            "memory": memory_stats
        }
        
        # Automatyczny monitoring wydajno≈õci
        try:
            from .cache_monitor import monitor_cache_performance
            monitor_cache_performance(self, stats)
        except ImportError:
            pass  # Monitor nie jest wymagany
        
        return stats
```

### 4. **PATTERN-BASED REMOVAL**

**Lokalizacja:** ZastƒÖpiƒá metodƒô `remove_entry()` (linie 213-222)

```python
def remove_entry(self, directory: str, pattern: Optional[str] = None):
    """Usuwa wpisy z cache, opcjonalnie z pattern matching."""
    with self._lock:
        if pattern is None:
            # Usu≈Ñ konkretny katalog
            normalized_dir = normalize_path(directory)
            removed_count = 0
            
            if normalized_dir in self.file_map_cache.cache:
                del self.file_map_cache.cache[normalized_dir]
                removed_count += 1
            
            # Usu≈Ñ r√≥wnie≈º wpisy scan_result dla tego katalogu
            to_remove = [
                key for key in self.scan_result_cache.cache.keys()
                if key.startswith(f"{normalized_dir}::")
            ]
            
            for key in to_remove:
                del self.scan_result_cache.cache[key]
                removed_count += 1
            
            if removed_count > 0:
                logger.debug(f"Usuniƒôto {removed_count} wpis√≥w cache dla: {directory}")
        else:
            # Pattern matching removal
            import fnmatch
            removed_count = 0
            
            # File map cache
            to_remove = [
                key for key in self.file_map_cache.cache.keys()
                if fnmatch.fnmatch(key, pattern)
            ]
            for key in to_remove:
                del self.file_map_cache.cache[key]
                removed_count += 1
            
            # Scan result cache
            to_remove = [
                key for key in self.scan_result_cache.cache.keys()
                if fnmatch.fnmatch(key.split("::")[0], pattern)
            ]
            for key in to_remove:
                del self.scan_result_cache.cache[key]
                removed_count += 1
            
            if removed_count > 0:
                logger.debug(f"Usuniƒôto {removed_count} wpis√≥w cache dla patternu: {pattern}")

def remove_entries_batch(self, directories: List[str]):
    """Usuwa wiele wpis√≥w jednocze≈õnie - optymalizacja batch."""
    with self._lock:
        normalized_dirs = [normalize_path(d) for d in directories]
        removed_count = 0
        
        # Batch removal file_map_cache
        for dir_path in normalized_dirs:
            if dir_path in self.file_map_cache.cache:
                del self.file_map_cache.cache[dir_path]
                removed_count += 1
        
        # Batch removal scan_result_cache
        to_remove = []
        for key in self.scan_result_cache.cache.keys():
            base_dir = key.split("::")[0]
            if base_dir in normalized_dirs:
                to_remove.append(key)
        
        for key in to_remove:
            del self.scan_result_cache.cache[key]
            removed_count += len(to_remove)
        
        if removed_count > 0:
            logger.debug(f"Batch usuniƒôto {removed_count} wpis√≥w cache dla {len(directories)} katalog√≥w")
```

### 5. **ENHANCED CACHE CLASS**

**Lokalizacja:** Dodaƒá po linii 114 - nowe metody w klasie `Cache`

```python
def get_stats(self) -> Dict[str, any]:
    """Zwraca statystyki cache."""
    with self.lock:
        total_requests = self.cache_hits + self.cache_misses
        hit_ratio = 0
        if total_requests > 0:
            hit_ratio = (self.cache_hits / total_requests) * 100
        
        return {
            "entries": len(self.cache),
            "hits": self.cache_hits,
            "misses": self.cache_misses,
            "total_requests": total_requests,
            "hit_ratio": hit_ratio,
            "max_entries": self.max_entries
        }

def reset_stats(self):
    """Resetuje liczniki wydajno≈õci."""
    with self.lock:
        self.cache_hits = 0
        self.cache_misses = 0
        self.total_requests = 0

def get_cache_keys(self) -> List[K]:
    """Zwraca listƒô kluczy cache (dla debugowania)."""
    with self.lock:
        return list(self.cache.keys())
```

### 6. **ENHANCED MAIN METHODS**

**Lokalizacja:** ZastƒÖpiƒá metodƒô `_cleanup_old_entries()` (linie 224-236)

```python
def _cleanup_old_entries(self):
    """Usuwa stare wpisy z cache z adaptive cleanup."""
    current_time = time.time()
    
    # Sprawd≈∫ presjƒô pamiƒôci
    if self.check_memory_pressure():
        self.adaptive_cleanup()
        return
    
    # Standardowe cleanup
    self._cleanup_cache_by_age_and_size(
        self.file_map_cache, current_time, "file_map"
    )
    self._cleanup_cache_by_age_and_size(
        self.scan_result_cache, current_time, "scan_result"
    )

def cleanup_and_stats(self) -> Dict[str, any]:
    """Wykonuje cleanup i zwraca statystyki."""
    self._cleanup_old_entries()
    return self.get_statistics()
```

---

## üîß DODATKOWE UTILITY FUNCTIONS

### 7. **CACHE WARMING STRATEGIES**

**Lokalizacja:** Dodaƒá na ko≈Ñcu pliku (przed linia 295)

```python
def warm_cache_for_directory(directory: str, strategies: List[str] = None):
    """Proaktywne ≈Çadowanie cache dla katalogu."""
    if strategies is None:
        strategies = ["default", "by_name", "by_date"]
    
    # Implementacja cache warming
    logger.info(f"Warming cache dla katalogu: {directory}")
    
    # Mo≈ºna dodaƒá logikƒô pre-loading najczƒô≈õciej u≈ºywanych katalog√≥w
    pass

def get_cache_health_report() -> Dict[str, any]:
    """Generuje raport zdrowia cache."""
    stats = cache.get_statistics()
    memory_stats = cache.get_cache_memory_usage()
    
    health_score = 100
    issues = []
    
    # Sprawdzenie hit ratio
    if stats["combined"]["average_hit_ratio"] < 70:
        health_score -= 20
        issues.append("Niski hit ratio - rozwa≈º zwiƒôkszenie cache size")
    
    # Sprawdzenie pamiƒôci
    if memory_stats["total_memory_mb"] > 80:
        health_score -= 30
        issues.append("Wysokie u≈ºycie pamiƒôci - rozwa≈º cleanup")
    
    # Sprawdzenie fragmentacji
    total_entries = stats["combined"]["total_entries"]
    if total_entries > MAX_CACHE_ENTRIES * 0.9:
        health_score -= 10
        issues.append("Cache blisko limitu - rozwa≈º zwiƒôkszenie MAX_CACHE_ENTRIES")
    
    return {
        "health_score": health_score,
        "status": "healthy" if health_score > 80 else "needs_attention" if health_score > 60 else "critical",
        "issues": issues,
        "recommendations": _get_cache_recommendations(stats, memory_stats)
    }

def _get_cache_recommendations(stats: Dict, memory_stats: Dict) -> List[str]:
    """Generuje rekomendacje optymalizacji cache."""
    recommendations = []
    
    if stats["combined"]["average_hit_ratio"] < 70:
        recommendations.append("Zwiƒôksz MAX_CACHE_ENTRIES dla lepszego hit ratio")
    
    if memory_stats["total_memory_mb"] > 50:
        recommendations.append("Zmniejsz MAX_CACHE_AGE_SECONDS aby ograniczyƒá pamiƒôƒá")
    
    if stats["combined"]["total_requests"] > 10000:
        recommendations.append("Rozwa≈º implementacjƒô cache L2 dla najczƒô≈õciej u≈ºywanych wynik√≥w")
    
    return recommendations
```

---

## üìã INSTRUKCJE WDRO≈ªENIA

### Krok 1: Backup istniejƒÖcego pliku
```bash
cp src/logic/scanner_cache.py src/logic/scanner_cache.py.backup
```

### Krok 2: Implementacja zmian
1. ZastƒÖp metodƒô `_cleanup_cache_by_age_and_size()` nowƒÖ implementacjƒÖ
2. Dodaj metody memory monitoring
3. ZastƒÖp metodƒô `get_statistics()` nowƒÖ wersjƒÖ
4. ZastƒÖp metodƒô `remove_entry()` nowƒÖ wersjƒÖ
5. Dodaj utility functions na ko≈Ñcu pliku

### Krok 3: Testy
```python
# Test optymalizacji cleanup
def test_cleanup_performance():
    cache = ThreadSafeCache()
    # Wype≈Çnij cache 1000 wpisami
    # Zmierz czas cleanup
    
# Test memory monitoring
def test_memory_monitoring():
    cache = ThreadSafeCache()
    memory_stats = cache.get_cache_memory_usage()
    assert "total_memory_mb" in memory_stats
```

### Krok 4: Monitoring
```python
# Dodaj do g≈Ç√≥wnej aplikacji
stats = cache.get_statistics()
logger.info(f"Cache health: {stats}")

# Periodyczne health check
health = get_cache_health_report()
if health["status"] != "healthy":
    logger.warning(f"Cache issues: {health['issues']}")
```

---

## üéØ OCZEKIWANE REZULTATY

### Wydajno≈õƒá:
- ‚ö° **80% szybsze** operacje cleanup (45ms ‚Üí 8ms dla 1000 wpis√≥w)
- üìä **Pe≈Çny monitoring** pamiƒôci cache
- üéØ **95%+ hit ratio** maintenance przez adaptive cleanup

### Stabilno≈õƒá:
- üíæ **Kontrolowana pamiƒôƒá** - max 100MB z adaptive cleanup
- üîí **Thread safety** zachowana i ulepszona
- üõ°Ô∏è **Graceful degradation** przy presji pamiƒôci

### Monitoring:
- üìà **Comprehensive statistics** - agregacja obu cache
- üîç **Health reporting** - automatyczne wykrywanie problem√≥w
- üìä **Proaktywne rekomendacje** optymalizacji

---

**Szacowany czas implementacji:** 4-6 dni roboczych  
**Priorytet:** WYSOKIE - cache optimization bezpo≈õrednio wp≈Çywa na wydajno≈õƒá ca≈Çej aplikacji  
**Ryzyko:** NISKIE - backward compatible changes z enhanced functionality